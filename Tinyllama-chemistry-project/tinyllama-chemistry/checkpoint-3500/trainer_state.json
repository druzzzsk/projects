{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.8,
  "eval_steps": 500,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.016,
      "grad_norm": 0.46195682883262634,
      "learning_rate": 0.00019898666666666668,
      "loss": 0.9626,
      "step": 20
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.4808582067489624,
      "learning_rate": 0.00019792000000000003,
      "loss": 0.9395,
      "step": 40
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.43409043550491333,
      "learning_rate": 0.00019685333333333333,
      "loss": 0.9499,
      "step": 60
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.463716596364975,
      "learning_rate": 0.00019578666666666668,
      "loss": 0.9451,
      "step": 80
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.4445379674434662,
      "learning_rate": 0.00019472,
      "loss": 0.9485,
      "step": 100
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.44913172721862793,
      "learning_rate": 0.00019365333333333336,
      "loss": 0.9335,
      "step": 120
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.4811331629753113,
      "learning_rate": 0.00019258666666666668,
      "loss": 0.9312,
      "step": 140
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.4502563178539276,
      "learning_rate": 0.00019152,
      "loss": 0.9384,
      "step": 160
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.460163950920105,
      "learning_rate": 0.00019045333333333336,
      "loss": 0.9301,
      "step": 180
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.4813917577266693,
      "learning_rate": 0.00018938666666666666,
      "loss": 0.9348,
      "step": 200
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.5132015943527222,
      "learning_rate": 0.00018832,
      "loss": 0.9362,
      "step": 220
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.47315704822540283,
      "learning_rate": 0.00018725333333333334,
      "loss": 0.9243,
      "step": 240
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.4695524275302887,
      "learning_rate": 0.00018618666666666666,
      "loss": 0.9335,
      "step": 260
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.4497581720352173,
      "learning_rate": 0.00018512000000000002,
      "loss": 0.9125,
      "step": 280
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.4573979675769806,
      "learning_rate": 0.00018405333333333334,
      "loss": 0.9126,
      "step": 300
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.4448581039905548,
      "learning_rate": 0.0001829866666666667,
      "loss": 0.9064,
      "step": 320
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.48653483390808105,
      "learning_rate": 0.00018192,
      "loss": 0.9187,
      "step": 340
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.4503934979438782,
      "learning_rate": 0.00018085333333333335,
      "loss": 0.9174,
      "step": 360
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.48614951968193054,
      "learning_rate": 0.00017978666666666667,
      "loss": 0.9128,
      "step": 380
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.49193841218948364,
      "learning_rate": 0.00017872,
      "loss": 0.9096,
      "step": 400
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.49450358748435974,
      "learning_rate": 0.00017765333333333335,
      "loss": 0.9166,
      "step": 420
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.5011053681373596,
      "learning_rate": 0.00017658666666666667,
      "loss": 0.903,
      "step": 440
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.48006272315979004,
      "learning_rate": 0.00017552000000000003,
      "loss": 0.9137,
      "step": 460
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.46525776386260986,
      "learning_rate": 0.00017445333333333333,
      "loss": 0.9343,
      "step": 480
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5374618172645569,
      "learning_rate": 0.00017338666666666668,
      "loss": 0.893,
      "step": 500
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.5194789171218872,
      "learning_rate": 0.00017232,
      "loss": 0.8852,
      "step": 520
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.45894140005111694,
      "learning_rate": 0.00017125333333333333,
      "loss": 0.8979,
      "step": 540
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.47057992219924927,
      "learning_rate": 0.00017018666666666668,
      "loss": 0.9027,
      "step": 560
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.45551472902297974,
      "learning_rate": 0.00016912,
      "loss": 0.8873,
      "step": 580
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.47920191287994385,
      "learning_rate": 0.00016805333333333336,
      "loss": 0.9051,
      "step": 600
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.48091787099838257,
      "learning_rate": 0.00016698666666666666,
      "loss": 0.881,
      "step": 620
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.4809000492095947,
      "learning_rate": 0.00016592,
      "loss": 0.8847,
      "step": 640
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.48434871435165405,
      "learning_rate": 0.00016485333333333334,
      "loss": 0.8885,
      "step": 660
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.5123434662818909,
      "learning_rate": 0.00016378666666666666,
      "loss": 0.8965,
      "step": 680
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.4915481209754944,
      "learning_rate": 0.00016272000000000001,
      "loss": 0.8881,
      "step": 700
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.4724832773208618,
      "learning_rate": 0.00016165333333333334,
      "loss": 0.8906,
      "step": 720
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.5155362486839294,
      "learning_rate": 0.0001605866666666667,
      "loss": 0.8898,
      "step": 740
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.5166449546813965,
      "learning_rate": 0.00015952,
      "loss": 0.8835,
      "step": 760
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.4825758635997772,
      "learning_rate": 0.00015845333333333334,
      "loss": 0.8701,
      "step": 780
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.4877535402774811,
      "learning_rate": 0.0001573866666666667,
      "loss": 0.8924,
      "step": 800
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.495613694190979,
      "learning_rate": 0.00015632,
      "loss": 0.8962,
      "step": 820
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.49199607968330383,
      "learning_rate": 0.00015525333333333335,
      "loss": 0.8924,
      "step": 840
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.5244290232658386,
      "learning_rate": 0.00015418666666666667,
      "loss": 0.8874,
      "step": 860
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.49314579367637634,
      "learning_rate": 0.00015312,
      "loss": 0.8765,
      "step": 880
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.5002285838127136,
      "learning_rate": 0.00015205333333333332,
      "loss": 0.9043,
      "step": 900
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.5782228112220764,
      "learning_rate": 0.00015098666666666668,
      "loss": 0.8695,
      "step": 920
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.5111019611358643,
      "learning_rate": 0.00014992000000000003,
      "loss": 0.8823,
      "step": 940
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.45097222924232483,
      "learning_rate": 0.00014885333333333333,
      "loss": 0.9082,
      "step": 960
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.47096630930900574,
      "learning_rate": 0.00014778666666666668,
      "loss": 0.8643,
      "step": 980
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.46543073654174805,
      "learning_rate": 0.00014672,
      "loss": 0.8882,
      "step": 1000
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.473507285118103,
      "learning_rate": 0.00014565333333333333,
      "loss": 0.8748,
      "step": 1020
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.5163503885269165,
      "learning_rate": 0.00014458666666666668,
      "loss": 0.886,
      "step": 1040
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.49002739787101746,
      "learning_rate": 0.00014352,
      "loss": 0.8923,
      "step": 1060
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.4614190459251404,
      "learning_rate": 0.00014245333333333336,
      "loss": 0.863,
      "step": 1080
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.4394322335720062,
      "learning_rate": 0.00014138666666666666,
      "loss": 0.8546,
      "step": 1100
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.4580734372138977,
      "learning_rate": 0.00014032,
      "loss": 0.8774,
      "step": 1120
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.5371724963188171,
      "learning_rate": 0.00013925333333333334,
      "loss": 0.8704,
      "step": 1140
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.5147033929824829,
      "learning_rate": 0.00013818666666666666,
      "loss": 0.8521,
      "step": 1160
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.502913236618042,
      "learning_rate": 0.00013712000000000002,
      "loss": 0.8778,
      "step": 1180
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.5598601698875427,
      "learning_rate": 0.00013605333333333334,
      "loss": 0.8711,
      "step": 1200
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.4384300112724304,
      "learning_rate": 0.0001349866666666667,
      "loss": 0.8651,
      "step": 1220
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.5012655854225159,
      "learning_rate": 0.00013392,
      "loss": 0.8678,
      "step": 1240
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.42943137884140015,
      "learning_rate": 0.00013285333333333334,
      "loss": 0.8508,
      "step": 1260
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.5025869607925415,
      "learning_rate": 0.00013178666666666667,
      "loss": 0.8481,
      "step": 1280
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.5197601318359375,
      "learning_rate": 0.00013072,
      "loss": 0.8488,
      "step": 1300
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.48178374767303467,
      "learning_rate": 0.00012965333333333335,
      "loss": 0.8435,
      "step": 1320
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.47892478108406067,
      "learning_rate": 0.00012858666666666667,
      "loss": 0.8732,
      "step": 1340
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.5226622819900513,
      "learning_rate": 0.00012752,
      "loss": 0.8599,
      "step": 1360
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.5163185596466064,
      "learning_rate": 0.00012645333333333332,
      "loss": 0.8512,
      "step": 1380
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.5038906931877136,
      "learning_rate": 0.00012538666666666668,
      "loss": 0.85,
      "step": 1400
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.5306482315063477,
      "learning_rate": 0.00012432,
      "loss": 0.8469,
      "step": 1420
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.514082133769989,
      "learning_rate": 0.00012325333333333333,
      "loss": 0.8472,
      "step": 1440
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.5713953375816345,
      "learning_rate": 0.00012218666666666668,
      "loss": 0.8515,
      "step": 1460
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.5219981670379639,
      "learning_rate": 0.00012112,
      "loss": 0.854,
      "step": 1480
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.4863220155239105,
      "learning_rate": 0.00012005333333333333,
      "loss": 0.8559,
      "step": 1500
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.48236438632011414,
      "learning_rate": 0.00011898666666666667,
      "loss": 0.8492,
      "step": 1520
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.45755812525749207,
      "learning_rate": 0.00011792000000000001,
      "loss": 0.8481,
      "step": 1540
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.5502204895019531,
      "learning_rate": 0.00011685333333333335,
      "loss": 0.8496,
      "step": 1560
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.4609394669532776,
      "learning_rate": 0.00011578666666666666,
      "loss": 0.8556,
      "step": 1580
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.4702572524547577,
      "learning_rate": 0.00011472,
      "loss": 0.8305,
      "step": 1600
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.5370656847953796,
      "learning_rate": 0.00011365333333333334,
      "loss": 0.8496,
      "step": 1620
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.5645560026168823,
      "learning_rate": 0.00011258666666666666,
      "loss": 0.8398,
      "step": 1640
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.4941374659538269,
      "learning_rate": 0.00011152,
      "loss": 0.8671,
      "step": 1660
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.5035735964775085,
      "learning_rate": 0.00011045333333333334,
      "loss": 0.8531,
      "step": 1680
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.5003780722618103,
      "learning_rate": 0.00010938666666666668,
      "loss": 0.8472,
      "step": 1700
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.490611732006073,
      "learning_rate": 0.00010831999999999999,
      "loss": 0.8478,
      "step": 1720
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.5352052450180054,
      "learning_rate": 0.00010725333333333335,
      "loss": 0.8443,
      "step": 1740
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.5263198614120483,
      "learning_rate": 0.00010618666666666668,
      "loss": 0.8418,
      "step": 1760
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.4760829508304596,
      "learning_rate": 0.00010512,
      "loss": 0.8461,
      "step": 1780
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.5354615449905396,
      "learning_rate": 0.00010405333333333334,
      "loss": 0.8552,
      "step": 1800
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.5380223393440247,
      "learning_rate": 0.00010298666666666667,
      "loss": 0.8475,
      "step": 1820
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.579001784324646,
      "learning_rate": 0.00010192000000000001,
      "loss": 0.8627,
      "step": 1840
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.5159475207328796,
      "learning_rate": 0.00010085333333333334,
      "loss": 0.8411,
      "step": 1860
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.5332479476928711,
      "learning_rate": 9.978666666666668e-05,
      "loss": 0.8677,
      "step": 1880
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.5670449733734131,
      "learning_rate": 9.872e-05,
      "loss": 0.8399,
      "step": 1900
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.47434937953948975,
      "learning_rate": 9.765333333333334e-05,
      "loss": 0.8257,
      "step": 1920
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.49864253401756287,
      "learning_rate": 9.658666666666667e-05,
      "loss": 0.8367,
      "step": 1940
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.5266073942184448,
      "learning_rate": 9.552000000000001e-05,
      "loss": 0.8456,
      "step": 1960
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.5218087434768677,
      "learning_rate": 9.445333333333333e-05,
      "loss": 0.8405,
      "step": 1980
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.5063969492912292,
      "learning_rate": 9.338666666666667e-05,
      "loss": 0.8444,
      "step": 2000
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.4859112799167633,
      "learning_rate": 9.232000000000001e-05,
      "loss": 0.8269,
      "step": 2020
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.5061607360839844,
      "learning_rate": 9.125333333333334e-05,
      "loss": 0.842,
      "step": 2040
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.4954535663127899,
      "learning_rate": 9.018666666666668e-05,
      "loss": 0.8461,
      "step": 2060
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.5328149199485779,
      "learning_rate": 8.912e-05,
      "loss": 0.8361,
      "step": 2080
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.5167034864425659,
      "learning_rate": 8.805333333333333e-05,
      "loss": 0.8431,
      "step": 2100
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.49080437421798706,
      "learning_rate": 8.698666666666668e-05,
      "loss": 0.8421,
      "step": 2120
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.5117960572242737,
      "learning_rate": 8.592e-05,
      "loss": 0.8497,
      "step": 2140
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.5014623403549194,
      "learning_rate": 8.485333333333334e-05,
      "loss": 0.8539,
      "step": 2160
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.5315271615982056,
      "learning_rate": 8.378666666666667e-05,
      "loss": 0.8409,
      "step": 2180
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.5435019135475159,
      "learning_rate": 8.272000000000001e-05,
      "loss": 0.8465,
      "step": 2200
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.5532320737838745,
      "learning_rate": 8.165333333333333e-05,
      "loss": 0.8348,
      "step": 2220
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.5745091438293457,
      "learning_rate": 8.058666666666667e-05,
      "loss": 0.8409,
      "step": 2240
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.4916767477989197,
      "learning_rate": 7.952000000000001e-05,
      "loss": 0.8345,
      "step": 2260
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.5674288868904114,
      "learning_rate": 7.845333333333334e-05,
      "loss": 0.8421,
      "step": 2280
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.4716416597366333,
      "learning_rate": 7.738666666666668e-05,
      "loss": 0.8651,
      "step": 2300
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.4913893938064575,
      "learning_rate": 7.632e-05,
      "loss": 0.8345,
      "step": 2320
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.5211286544799805,
      "learning_rate": 7.525333333333334e-05,
      "loss": 0.8244,
      "step": 2340
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.5684869289398193,
      "learning_rate": 7.418666666666667e-05,
      "loss": 0.8372,
      "step": 2360
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.5504117608070374,
      "learning_rate": 7.312e-05,
      "loss": 0.8471,
      "step": 2380
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.5358750224113464,
      "learning_rate": 7.205333333333334e-05,
      "loss": 0.8441,
      "step": 2400
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.5345633029937744,
      "learning_rate": 7.098666666666667e-05,
      "loss": 0.838,
      "step": 2420
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.5229633450508118,
      "learning_rate": 6.992000000000001e-05,
      "loss": 0.8278,
      "step": 2440
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.47919750213623047,
      "learning_rate": 6.885333333333333e-05,
      "loss": 0.821,
      "step": 2460
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.5161451101303101,
      "learning_rate": 6.778666666666666e-05,
      "loss": 0.8367,
      "step": 2480
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.5466181635856628,
      "learning_rate": 6.672e-05,
      "loss": 0.8317,
      "step": 2500
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.5139511227607727,
      "learning_rate": 6.565333333333334e-05,
      "loss": 0.8179,
      "step": 2520
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.5198957324028015,
      "learning_rate": 6.458666666666668e-05,
      "loss": 0.8152,
      "step": 2540
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.534232497215271,
      "learning_rate": 6.352e-05,
      "loss": 0.8348,
      "step": 2560
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.5506647229194641,
      "learning_rate": 6.245333333333334e-05,
      "loss": 0.8264,
      "step": 2580
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.57602459192276,
      "learning_rate": 6.138666666666667e-05,
      "loss": 0.8385,
      "step": 2600
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.49494388699531555,
      "learning_rate": 6.032e-05,
      "loss": 0.8413,
      "step": 2620
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.47626352310180664,
      "learning_rate": 5.925333333333334e-05,
      "loss": 0.8376,
      "step": 2640
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.5901386737823486,
      "learning_rate": 5.818666666666667e-05,
      "loss": 0.8419,
      "step": 2660
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.48770198225975037,
      "learning_rate": 5.712000000000001e-05,
      "loss": 0.821,
      "step": 2680
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.5302364826202393,
      "learning_rate": 5.6053333333333334e-05,
      "loss": 0.8161,
      "step": 2700
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.594795286655426,
      "learning_rate": 5.4986666666666666e-05,
      "loss": 0.8154,
      "step": 2720
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.5130077004432678,
      "learning_rate": 5.3920000000000006e-05,
      "loss": 0.8507,
      "step": 2740
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.5505049228668213,
      "learning_rate": 5.285333333333333e-05,
      "loss": 0.835,
      "step": 2760
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.5223692059516907,
      "learning_rate": 5.178666666666667e-05,
      "loss": 0.8266,
      "step": 2780
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.5056882500648499,
      "learning_rate": 5.072e-05,
      "loss": 0.8426,
      "step": 2800
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.5390818119049072,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.8344,
      "step": 2820
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.535884439945221,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.8175,
      "step": 2840
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.49769893288612366,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.8138,
      "step": 2860
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.5542879104614258,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.8396,
      "step": 2880
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.5165773034095764,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.8183,
      "step": 2900
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.5332098603248596,
      "learning_rate": 4.432e-05,
      "loss": 0.8131,
      "step": 2920
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.5030264854431152,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.835,
      "step": 2940
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.4922005236148834,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.8181,
      "step": 2960
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.5616639852523804,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.8357,
      "step": 2980
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.5288341641426086,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.819,
      "step": 3000
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.5338739156723022,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.8237,
      "step": 3020
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.5184572339057922,
      "learning_rate": 3.792e-05,
      "loss": 0.8193,
      "step": 3040
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.5222752690315247,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.8442,
      "step": 3060
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.5427281856536865,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.8199,
      "step": 3080
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.5595864653587341,
      "learning_rate": 3.472e-05,
      "loss": 0.805,
      "step": 3100
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.5703592896461487,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.8217,
      "step": 3120
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.5582293272018433,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.8088,
      "step": 3140
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.5340151786804199,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.8248,
      "step": 3160
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.5150298476219177,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.8303,
      "step": 3180
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.5923822522163391,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.8259,
      "step": 3200
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.5132843852043152,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.8157,
      "step": 3220
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.5087240934371948,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.8336,
      "step": 3240
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.5196730494499207,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.8306,
      "step": 3260
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.5036892890930176,
      "learning_rate": 2.512e-05,
      "loss": 0.8236,
      "step": 3280
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.5679389834403992,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.8146,
      "step": 3300
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.545992910861969,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.835,
      "step": 3320
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.5214282870292664,
      "learning_rate": 2.192e-05,
      "loss": 0.8192,
      "step": 3340
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.5592796802520752,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.8344,
      "step": 3360
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.535113513469696,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.817,
      "step": 3380
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.5187262892723083,
      "learning_rate": 1.872e-05,
      "loss": 0.8146,
      "step": 3400
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.5220168232917786,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.8305,
      "step": 3420
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.56016606092453,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.8347,
      "step": 3440
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.4578099548816681,
      "learning_rate": 1.552e-05,
      "loss": 0.8313,
      "step": 3460
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.4716953933238983,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.8232,
      "step": 3480
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.5270412564277649,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.8185,
      "step": 3500
    }
  ],
  "logging_steps": 20,
  "max_steps": 3750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.33622258466816e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
